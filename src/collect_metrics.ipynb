{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import  classification_report\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.remove_pipe(\"lemmatizer\")\n",
    "nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"}).initialize()\n",
    "processor=AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics-80b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(h, r, sent_len=None):\n",
    "    #Reference: https://github.com/zszyellow/WER-in-python/blob/master/wer.py\n",
    "    '''\n",
    "    This function is to calculate the edit distance of reference sentence and the hypothesis sentence.\n",
    "\n",
    "    Main algorithm used is dynamic programming.\n",
    "\n",
    "    Attributes: \n",
    "        r -> the list of words produced by splitting reference sentence.\n",
    "        h -> the list of words produced by splitting hypothesis sentence.\n",
    "    '''\n",
    "    d = np.zeros((len(r)+1)*(len(h)+1), dtype=np.uint8).reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(h)+1):\n",
    "        d[0][j] = j\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitute = d[i-1][j-1]+1\n",
    "                insert = d[i][j-1]+1 \n",
    "                delete = d[i-1][j]\n",
    "                d[i][j] = min(substitute, insert, delete)\n",
    "    # d=d[len(r)][len(h)]/len(r)\n",
    "    d=d[len(r)][len(h)]\n",
    "    \n",
    "    if len(r)!=0:\n",
    "        wnr=d/len(r)\n",
    "        \n",
    "    else:\n",
    "        wnr=1 \n",
    "\n",
    "    return d, wnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_msg_distance(sentences):\n",
    " \n",
    "    lemmatized_sentences = []\n",
    "    sentences_lemma=[]\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().strip('.').lower()\n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "     \n",
    "        filtered_sentence_lemma = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", 'PROPN','NUM','PRON','ADP']:\n",
    "                token_lemma=token.lemma_\n",
    "                filtered_sentence_lemma.append(token_lemma)\n",
    "          \n",
    "        lemmatized_sentences.append(' '.join(filtered_sentence_lemma))\n",
    "        sentences_lemma.append(filtered_sentence_lemma)\n",
    "    \n",
    "\n",
    "\n",
    "    wnd, wnr=editDistance(sentences_lemma[1], sentences_lemma[0])\n",
    "\n",
    "\n",
    "\n",
    "    return wnd, wnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_metrics(exp_type, model_type, speaker_result_fp, msg_type, output_dir):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print('-'*20)\n",
    "    print(speaker_result_fp)\n",
    "    all_results=glob(speaker_result_fp)\n",
    "    print(len(all_results))\n",
    "\n",
    "    msg_len_data=[]\n",
    "    msg_wnd_data=[]\n",
    "    msg_wnr_data=[]\n",
    "\n",
    "\n",
    "    for result in all_results:\n",
    "        # print(result)\n",
    "        utter_len=[]\n",
    "        utter_wnd=[]\n",
    "        utter_wnr=[]\n",
    "\n",
    "        df=pd.read_csv(result, index_col=0)\n",
    "\n",
    "        \n",
    "        for utter in df[msg_type]:\n",
    "            utter_len.append(len(processor.tokenize(utter)))\n",
    "\n",
    "        \n",
    "        for targetName in df.targetImg.unique():\n",
    "            sub_df=df[df['targetImg']==targetName]\n",
    "            Img_utter_wer=[]\n",
    "            Img_utter_wnr=[]\n",
    "            prev_utter=None\n",
    "   \n",
    "            for utter in sub_df[msg_type]:\n",
    "                if prev_utter:\n",
    "                    msg_wnd, msg_wnr=get_msg_distance([prev_utter, utter])\n",
    "                    Img_utter_wer.append(msg_wnd)\n",
    "                    Img_utter_wnr.append(msg_wnr)\n",
    "                prev_utter=utter\n",
    "                \n",
    "            utter_wnd.append(Img_utter_wer)\n",
    "            utter_wnr.append(Img_utter_wnr)\n",
    "\n",
    "\n",
    "        \n",
    "        msg_len_data.append(utter_len)\n",
    "        msg_wnd_data.append(utter_wnd)\n",
    "        msg_wnr_data.append(utter_wnr)\n",
    "\n",
    "\n",
    "    msg_len_data=np.array(msg_len_data).reshape(-1, 6, 4).mean(axis=2)\n",
    "    msg_wnd_data=np.array(msg_wnd_data).mean(axis=1)\n",
    "    msg_wnr_data=np.array(msg_wnr_data).mean(axis=1)\n",
    "\n",
    "    # np.save(f'{output_dir}/{exp_type}_{model_type}_msg_len_data.npy', msg_len_data)\n",
    "    # np.save(f'{output_dir}/{exp_type}_{model_type}_msg_wnd_data.npy', msg_wnd_data)\n",
    "    # np.save(f'{output_dir}/{exp_type}_{model_type}_msg_wnr_data.npy', msg_wnr_data)\n",
    "    msg_len_avg=msg_len_data.mean(axis=0)\n",
    "    msg_wnd_avg=msg_wnd_data.mean(axis=0)\n",
    "    msg_wnr_avg=msg_wnr_data.mean(axis=0)\n",
    "\n",
    "    print(\"Msg Avg Length: \", msg_len_avg)\n",
    "    print(\"Msg Avg WND: \", msg_wnd_avg)\n",
    "    print(\"Msg Avg WNR: \", msg_wnr_avg)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(exp_type, model_type, result_fp, target_column,pred_column, output_dir, first_round=1, last_round=6):\n",
    "    print(result_fp)\n",
    "    all_fps=glob(result_fp)\n",
    "    print(len(all_fps))\n",
    "    all_data_logs=[]\n",
    "    for fp in all_fps:\n",
    "    \n",
    "        df=pd.read_csv(fp)\n",
    "        interaction_logs=defaultdict(list)\n",
    "        for i in range(first_round-1, last_round):\n",
    "            golds=[]\n",
    "            preds=[]      \n",
    "            golds.append(df[target_column][4*i:4*(i+1)])\n",
    "            preds.append(df[pred_column][4*i:4*(i+1)])\n",
    "\n",
    "            golds=pd.concat(golds)\n",
    "            preds=pd.concat(preds)\n",
    "\n",
    "            \n",
    "            report=classification_report(golds, preds, zero_division=0, output_dict=True)\n",
    "         \n",
    "           \n",
    "\n",
    "            data_to_log={'rep_acc':report['accuracy']}\n",
    "        \n",
    "            for k, v in report.items():\n",
    "                if k=='accuracy':\n",
    "                    continue\n",
    "                for kk, vv in v.items():\n",
    "                    data_to_log[k+'_'+kk]=vv\n",
    "            \n",
    "            for k, v in data_to_log.items():\n",
    "                interaction_logs[k].append(v)\n",
    "        all_data_logs.append(interaction_logs)\n",
    "\n",
    "   \n",
    "    df=pd.DataFrame(all_data_logs)\n",
    "    # df.to_csv(f'{output_dir}/accuracies_{exp_type}_{model_type}.csv')  \n",
    "    accuracies=np.array(list(df['rep_acc']), dtype=float)\n",
    "    mean_accuracy=accuracies.mean(axis=0)\n",
    "    print(mean_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type='explicitconsistency'\n",
    "model_type='Claude' \n",
    "output_dir='evaluation_results'\n",
    "pred_column='lsnr_pred'\n",
    "target_column='tgt_label_for_lsnr'\n",
    "msg_type='spkr_msg'\n",
    "fp=\"outputs_icca/test/hard_all/*/records_XXXX_output.csv\"\n",
    "\n",
    "get_accuracies(exp_type, model_type, fp, target_column=target_column,pred_column=pred_column, output_dir=output_dir)\n",
    "get_speaker_metrics(exp_type, model_type, fp, msg_type, output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
